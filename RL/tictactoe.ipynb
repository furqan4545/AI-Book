{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b1caf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d35df252",
   "metadata": {},
   "source": [
    "# Working code, computer vs computer AI agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d9f0b0a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T21:07:05.855455Z",
     "start_time": "2023-04-03T21:05:52.799544Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: winner = 2, epsilon = 0.899\n",
      "Episode 10000: winner = 1, epsilon = 0.000\n",
      "Episode 20000: winner = 1, epsilon = 0.000\n",
      "Episode 30000: winner = 1, epsilon = 0.000\n",
      "Episode 40000: winner = 1, epsilon = 0.000\n",
      "Episode 50000: winner = 1, epsilon = 0.000\n",
      "Episode 60000: winner = 1, epsilon = 0.000\n",
      "Episode 70000: winner = 1, epsilon = 0.000\n",
      "Episode 80000: winner = 1, epsilon = 0.000\n",
      "Episode 90000: winner = 1, epsilon = 0.000\n",
      "Agent 1 win rate: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the Tic-Tac-Toe board\n",
    "board_rows = 3\n",
    "board_cols = 3\n",
    "\n",
    "# Define the player markers\n",
    "player_1 = 1\n",
    "player_2 = 2\n",
    "\n",
    "# Define the winning combinations\n",
    "winning_combinations = [\n",
    "    [0, 1, 2],\n",
    "    [3, 4, 5],\n",
    "    [6, 7, 8],\n",
    "    [0, 3, 6],\n",
    "    [1, 4, 7],\n",
    "    [2, 5, 8],\n",
    "    [0, 4, 8],\n",
    "    [2, 4, 6],\n",
    "]\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha, gamma, epsilon, learning_rate, discount_factor, player_marker):\n",
    "        self.q_table = {}\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.player_marker = player_marker\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        # If the state doesn't exist in the q_table, create a new entry\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(board_rows * board_cols)\n",
    "\n",
    "        return self.q_table[state][action]\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        # Convert the state to a bytes object\n",
    "        state_bytes = bytes(state)\n",
    "\n",
    "        # Add a new entry to the Q-table if the current state is not present\n",
    "        if state_bytes not in self.q_table:\n",
    "            self.q_table[state_bytes] = np.zeros(board_rows * board_cols)\n",
    "\n",
    "        # Choose a random action with probability epsilon\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.randint(board_rows * board_cols)\n",
    "        else:\n",
    "            # Choose the action with the highest Q-value\n",
    "            q_values = self.q_table[state_bytes]\n",
    "            max_q_value = np.max(q_values)\n",
    "            actions = np.where(q_values == max_q_value)[0]\n",
    "            action = np.random.choice(actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def update_q_value(self, state, action, next_state, reward):\n",
    "        # Get the current Q-value\n",
    "        q_value = self.get_q_value(state, action)\n",
    "\n",
    "        # Get the maximum Q-value for the next state\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros(board_rows * board_cols)\n",
    "        max_q_value = np.max(self.q_table[next_state])\n",
    "\n",
    "        # Update the Q-value using the Bellman equation\n",
    "        new_q_value = q_value + self.alpha * (reward + self.gamma * max_q_value - q_value)\n",
    "\n",
    "        # Update the Q-table\n",
    "        self.q_table[state][action] = new_q_value\n",
    "\n",
    "class TicTacToeGame:\n",
    "    def __init__(self, player_1, player_2):\n",
    "        self.board = np.zeros((board_rows, board_cols))\n",
    "        self.player_1 = player_1\n",
    "        self.player_2 = player_2\n",
    "        self.current_player = self.player_1\n",
    "        self.winner = None\n",
    "\n",
    "    def get_state(self):\n",
    "        # Convert the board into a string\n",
    "        return self.board.tobytes()\n",
    "\n",
    "    def get_valid_moves(self):\n",
    "        # Get the indices of all empty squares on the board\n",
    "        return np.where(self.board.flatten() == 0)[0]\n",
    "\n",
    "    def is_valid_move(self, move):\n",
    "        # Check if the move is valid\n",
    "        return move in self.get_valid_moves()\n",
    "\n",
    "    def make_move(self, move):\n",
    "        # Make a move on the board\n",
    "        row = move // board_cols\n",
    "        col = move % board_cols\n",
    "        self.board[row][col] = self.current_player\n",
    "\n",
    "    def switch_player(self):\n",
    "        # Switch to the next player\n",
    "        if self.current_player == self.player_1:\n",
    "            self.current_player = self.player_2\n",
    "        else:\n",
    "            self.current_player = self.player_1\n",
    "\n",
    "    def check_winner(self):\n",
    "        # Check if any player has won\n",
    "        for combination in winning_combinations:\n",
    "            if (self.board.flatten()[combination] == self.current_player).all():\n",
    "                self.winner = self.current_player\n",
    "                return True\n",
    "\n",
    "        # Check if the game is a draw\n",
    "        if len(self.get_valid_moves()) == 0:\n",
    "            self.winner = 0\n",
    "            return True\n",
    "\n",
    "        # If there is no winner or draw, return False\n",
    "        return False\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the board and current player\n",
    "        self.board = np.zeros((board_rows, board_cols))\n",
    "        self.current_player = self.player_1\n",
    "        self.winner = None\n",
    "\n",
    "    def play_game(self, agent_1, agent_2, train=True):\n",
    "        # Reset the game\n",
    "        self.reset()\n",
    "\n",
    "        # Loop until there is a winner or a draw\n",
    "        while not self.check_winner():\n",
    "            # Get the current state\n",
    "            state = self.get_state()\n",
    "\n",
    "            # Choose an action for the current player\n",
    "            if self.current_player == self.player_1:\n",
    "                action = agent_1.choose_action(state)\n",
    "            else:\n",
    "                action = agent_2.choose_action(state)\n",
    "\n",
    "            # Make the move on the board\n",
    "            self.make_move(action)\n",
    "\n",
    "            # Check if there is a winner or a draw\n",
    "            self.check_winner()\n",
    "\n",
    "            # Calculate the reward for the agents\n",
    "            if self.winner == agent_1.player_marker:\n",
    "                reward_1 = 1\n",
    "                reward_2 = -1\n",
    "            elif self.winner == agent_2.player_marker:\n",
    "                reward_1 = -1\n",
    "                reward_2 = 1\n",
    "            else:\n",
    "                reward_1 = 0\n",
    "                reward_2 = 0\n",
    "\n",
    "            # Update the Q-values for the agents\n",
    "            if train:\n",
    "                next_state = self.get_state()\n",
    "\n",
    "                agent_1.update_q_value(state, action, next_state, reward_1)\n",
    "                agent_2.update_q_value(state, action, next_state, reward_2)\n",
    "\n",
    "            # Switch to the next player\n",
    "            self.switch_player()\n",
    "\n",
    "        # Return the winner\n",
    "        return self.winner\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the agents\n",
    "    agent_1 = QLearningAgent(alpha = 0.5, gamma=0.9, epsilon=0.9, learning_rate=0.1, discount_factor=0.9,\n",
    "                             player_marker=player_1)\n",
    "    agent_2 = QLearningAgent(alpha = 0.5, gamma=0.9, epsilon=0.9, learning_rate=0.1, discount_factor=0.9,\n",
    "                             player_marker=player_2)\n",
    "\n",
    "    # Train the agents\n",
    "    num_episodes = 100000\n",
    "    for i in range(num_episodes):\n",
    "        game = TicTacToeGame(player_1, player_2)\n",
    "        winner = game.play_game(agent_1, agent_2)\n",
    "        agent_1.epsilon *= 0.999\n",
    "        agent_2.epsilon *= 0.999\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Episode {i}: winner = {winner}, epsilon = {agent_1.epsilon:.3f}\")\n",
    "\n",
    "    # Test the agents\n",
    "    num_games = 100\n",
    "    num_wins = 0\n",
    "    for i in range(num_games):\n",
    "        game = TicTacToeGame(player_1, player_2)\n",
    "        winner = game.play_game(agent_1, agent_2)\n",
    "        agent_1.epsilon *= 0.999\n",
    "        agent_2.epsilon *= 0.999\n",
    "\n",
    "        if winner == player_1:\n",
    "            num_wins += 1\n",
    "\n",
    "    print(f\"Agent 1 win rate: {num_wins / num_games * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d31a95d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c77dcb65",
   "metadata": {},
   "source": [
    "# Working code human with computer without GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b63324ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T03:19:57.963859Z",
     "start_time": "2023-04-05T03:18:06.717684Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your move (0-8): 0\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[1. 2. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Enter your move (0-8): 1\n",
      "Invalid move, please try again\n",
      "Enter your move (0-8): 3\n",
      "[[1. 2. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[1. 2. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 2.]]\n",
      "Enter your move (0-8): 6\n",
      "[[1. 2. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 2.]]\n",
      "Invalid move. Please choose a valid move.\n",
      "Invalid move. Please choose a valid move.\n",
      "[[1. 2. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 2. 2.]]\n",
      "[[1. 2. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 2. 2.]]\n",
      "You win!\n",
      "Episode 0: winner = None, epsilon = 0.000\n",
      "Enter your move (0-8): 1\n",
      "[[0. 1. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[0. 1. 0.]\n",
      " [0. 0. 0.]\n",
      " [2. 0. 0.]]\n",
      "Enter your move (0-8): 8\n",
      "[[0. 1. 0.]\n",
      " [0. 0. 0.]\n",
      " [2. 0. 1.]]\n",
      "[[0. 1. 0.]\n",
      " [0. 0. 0.]\n",
      " [2. 2. 1.]]\n",
      "Enter your move (0-8): 2\n",
      "[[0. 1. 1.]\n",
      " [0. 0. 0.]\n",
      " [2. 2. 1.]]\n",
      "Invalid move. Please choose a valid move.\n",
      "Invalid move. Please choose a valid move.\n",
      "Invalid move. Please choose a valid move.\n",
      "[[0. 1. 1.]\n",
      " [0. 0. 2.]\n",
      " [2. 2. 1.]]\n",
      "Enter your move (0-8): 0\n",
      "[[1. 1. 1.]\n",
      " [0. 0. 2.]\n",
      " [2. 2. 1.]]\n",
      "Invalid move. Please choose a valid move.\n",
      "Invalid move. Please choose a valid move.\n",
      "Invalid move. Please choose a valid move.\n",
      "[[1. 1. 1.]\n",
      " [0. 2. 2.]\n",
      " [2. 2. 1.]]\n",
      "[[1. 1. 1.]\n",
      " [0. 2. 2.]\n",
      " [2. 2. 1.]]\n",
      "You win!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [65]\u001b[0m, in \u001b[0;36m<cell line: 212>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m    220\u001b[0m     game \u001b[38;5;241m=\u001b[39m TicTacToeGame(player_1, player_2)\n\u001b[1;32m--> 221\u001b[0m     winner \u001b[38;5;241m=\u001b[39m \u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m     agent_1\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.999\u001b[39m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# agent_2.epsilon *= 0.999\u001b[39;00m\n",
      "Input \u001b[1;32mIn [65]\u001b[0m, in \u001b[0;36mTicTacToeGame.play_game\u001b[1;34m(self, agent_1, train)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m         move \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter your move (0-8): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_valid_move(move):\n\u001b[0;32m    158\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\furqan ali\\desktop\\python\\env\\lib\\site-packages\\ipykernel\\kernelbase.py:1161\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1160\u001b[0m     )\n\u001b[1;32m-> 1161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\furqan ali\\desktop\\python\\env\\lib\\site-packages\\ipykernel\\kernelbase.py:1205\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1202\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1203\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the Tic-Tac-Toe board\n",
    "board_rows = 3\n",
    "board_cols = 3\n",
    "\n",
    "# Define the player markers\n",
    "player_1 = 1\n",
    "player_2 = 2\n",
    "\n",
    "# Define the winning combinations\n",
    "winning_combinations = [\n",
    "    [0, 1, 2],\n",
    "    [3, 4, 5],\n",
    "    [6, 7, 8],\n",
    "    [0, 3, 6],\n",
    "    [1, 4, 7],\n",
    "    [2, 5, 8],\n",
    "    [0, 4, 8],\n",
    "    [2, 4, 6],\n",
    "]\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha, gamma, epsilon, learning_rate, discount_factor, player_marker):\n",
    "        self.q_table = {}\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.player_marker = player_marker\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        # If the state doesn't exist in the q_table, create a new entry\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(board_rows * board_cols)\n",
    "\n",
    "        return self.q_table[state][action]\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        # Convert the state to a bytes object\n",
    "        state_bytes = bytes(state)\n",
    "\n",
    "        # Add a new entry to the Q-table if the current state is not present\n",
    "        if state_bytes not in self.q_table:\n",
    "            self.q_table[state_bytes] = np.zeros(board_rows * board_cols)\n",
    "\n",
    "        # Choose a random action with probability epsilon\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.randint(board_rows * board_cols)\n",
    "        else:\n",
    "            # Choose the action with the highest Q-value\n",
    "            q_values = self.q_table[state_bytes]\n",
    "            max_q_value = np.max(q_values)\n",
    "            actions = np.where(q_values == max_q_value)[0]\n",
    "            action = np.random.choice(actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def update_q_value(self, state, action, next_state, reward):\n",
    "        # Get the current Q-value\n",
    "        q_value = self.get_q_value(state, action)\n",
    "\n",
    "        # Get the maximum Q-value for the next state\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros(board_rows * board_cols)\n",
    "        max_q_value = np.max(self.q_table[next_state])\n",
    "\n",
    "        # Update the Q-value using the Bellman equation\n",
    "        new_q_value = q_value + self.alpha * (reward + self.gamma * max_q_value - q_value)\n",
    "\n",
    "        # Update the Q-table\n",
    "        self.q_table[state][action] = new_q_value\n",
    "\n",
    "class TicTacToeGame:\n",
    "    def __init__(self, player_1, player_2):\n",
    "        self.board = np.zeros((board_rows, board_cols))\n",
    "        self.player_1 = player_1\n",
    "        self.player_2 = player_2\n",
    "        self.current_player = self.player_1\n",
    "        self.winner = None\n",
    "\n",
    "    def get_state(self):\n",
    "        # Convert the board into a string\n",
    "        return self.board.tobytes()\n",
    "\n",
    "    def get_valid_moves(self):\n",
    "        # Get the indices of all empty squares on the board\n",
    "        return np.where(self.board.flatten() == 0)[0]\n",
    "\n",
    "    def is_valid_move(self, move):\n",
    "        # Check if the move is valid\n",
    "        return move in self.get_valid_moves()\n",
    "\n",
    "    def make_move(self, move):\n",
    "        # Make a move on the board\n",
    "        row = move // board_cols\n",
    "        col = move % board_cols\n",
    "        if self.board[row][col] == 0:\n",
    "            self.board[row][col] = self.current_player\n",
    "        else:\n",
    "            while True:\n",
    "                print(\"Invalid move. Please choose a valid move.\")\n",
    "                move = self.current_player.choose_action(self.get_state())\n",
    "                row = move // board_cols\n",
    "                col = move % board_cols\n",
    "                if self.board[row][col] == 0:\n",
    "                    self.board[row][col] = self.current_player\n",
    "                    break\n",
    "        return\n",
    "\n",
    "    def switch_player(self):\n",
    "        # Switch to the next player\n",
    "        if self.current_player == self.player_1:\n",
    "            self.current_player = self.player_2\n",
    "        else:\n",
    "            self.current_player = self.player_1\n",
    "\n",
    "    def check_winner(self):\n",
    "        # Check if any player has won\n",
    "        for combination in winning_combinations:\n",
    "            if (self.board.flatten()[combination] == self.current_player).all():\n",
    "                self.winner = self.current_player\n",
    "                return True\n",
    "\n",
    "        # Check if the game is a draw\n",
    "        if len(self.get_valid_moves()) == 0:\n",
    "            self.winner = 0\n",
    "            return True\n",
    "\n",
    "        # If there is no winner or draw, return False\n",
    "        return False\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the board and current player\n",
    "        self.board = np.zeros((board_rows, board_cols))\n",
    "        self.current_player = self.player_1\n",
    "        self.winner = None\n",
    "\n",
    "    def play_game(self, agent_1, train=False):\n",
    "        # Reset the game\n",
    "        self.reset()\n",
    "\n",
    "        # Loop until there is a winner or a draw\n",
    "        while not self.check_winner():\n",
    "            # Get the current state\n",
    "            state = self.get_state()\n",
    "\n",
    "            # Choose an action for the current player\n",
    "            if self.current_player == player_1:\n",
    "                # Get the human player's move\n",
    "                while True:\n",
    "                    try:\n",
    "                        move = int(input(\"Enter your move (0-8): \"))\n",
    "                        if self.is_valid_move(move):\n",
    "                            break\n",
    "                        else:\n",
    "                            print(\"Invalid move, please try again\")\n",
    "                    except ValueError:\n",
    "                        print(\"Invalid input, please try again\")\n",
    "                action = move\n",
    "            else:\n",
    "                # Let the AI agent choose its move\n",
    "                action = agent_1.choose_action(state)\n",
    "                while not self.is_valid_move(action):\n",
    "                    print(\"Invalid move. Please choose a valid move.\")\n",
    "                    action = agent_1.choose_action(state)\n",
    "\n",
    "            # Make the move on the board\n",
    "            self.make_move(action)\n",
    "\n",
    "            # Check if there is a winner or a draw\n",
    "            self.check_winner()\n",
    "\n",
    "            # Print the current board\n",
    "            print(self.board)\n",
    "\n",
    "            # Calculate the reward for the agents\n",
    "            if self.winner == player_1:\n",
    "                reward_1 = 1\n",
    "                reward_2 = -1\n",
    "            elif self.winner == player_2:\n",
    "                reward_1 = -1\n",
    "                reward_2 = 1\n",
    "            else:\n",
    "                reward_1 = 0\n",
    "                reward_2 = 0\n",
    "\n",
    "            # Update the Q-values for the agents\n",
    "            if train and self.winner is not None:\n",
    "                next_state = self.get_state()\n",
    "\n",
    "                agent_1.update_q_value(state, action, next_state, reward_1)\n",
    "\n",
    "            # Switch to the next player\n",
    "            self.switch_player()\n",
    "\n",
    "        # Print the final board\n",
    "        print(self.board)\n",
    "\n",
    "        # Print the winner\n",
    "        if self.winner == player_1:\n",
    "            print(\"You win!\")\n",
    "        elif self.winner == player_2:\n",
    "            print(\"AI wins!\")\n",
    "        else:\n",
    "            print(\"Draw!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the agents\n",
    "    agent_1 = QLearningAgent(alpha=0.5, gamma=0.9, epsilon=0.0, learning_rate=0.1, discount_factor=0.9,\n",
    "                             player_marker=player_2)\n",
    "\n",
    "    # Train the agents\n",
    "    num_episodes = 100000\n",
    "    for i in range(num_episodes):\n",
    "        game = TicTacToeGame(player_1, player_2)\n",
    "        winner = game.play_game(agent_1)\n",
    "        agent_1.epsilon *= 0.999\n",
    "        # agent_2.epsilon *= 0.999\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Episode {i}: winner = {winner}, epsilon = {agent_1.epsilon:.3f}\")\n",
    "\n",
    "    # Test the agents\n",
    "    num_games = 100\n",
    "    num_wins = 0\n",
    "    for i in range(num_games):\n",
    "        game = TicTacToeGame(player_1, player_2)\n",
    "        winner = game.play_game(agent_1)\n",
    "        agent_1.epsilon *= 0.999\n",
    "        # agent_2.epsilon *= 0.999\n",
    "\n",
    "        if winner == player_1:\n",
    "            num_wins += 1\n",
    "\n",
    "    print(f\"Agent 1 win rate: {num_wins / num_games * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75046966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4a6dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will work on GUI code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641ea669",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-26T15:02:05.137Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e08a7804",
   "metadata": {},
   "source": [
    "# Simple GUI tic-tac-toe game without AI agent in it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb56ecd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T11:16:10.111616Z",
     "start_time": "2023-05-28T11:16:03.742519Z"
    }
   },
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.root = tk.Tk()\n",
    "        self.board = [[None for _ in range(3)] for _ in range(3)]\n",
    "        self.player_turn = 1  # Keep track of whose turn it is. 1 for player 1 and 2 for player 2.\n",
    "        self.game_over = False  # Indicates whether the game has ended\n",
    "        self.initialize_gui()\n",
    "\n",
    "    def initialize_gui(self):\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                button = tk.Button(self.root, text=\"\", font=(\"Arial\", 50), height=3, width=6,\n",
    "                                   command=lambda i=i, j=j: self.button_click(i, j))\n",
    "                button.grid(row=i, column=j)\n",
    "                self.board[i][j] = button\n",
    "\n",
    "    def button_click(self, i, j):\n",
    "        if not self.game_over and self.board[i][j]['text'] == \"\":  # Only allow moves on empty squares\n",
    "            marker = 'X' if self.player_turn == 1 else 'O'\n",
    "            self.board[i][j].config(text=marker, state='disabled')\n",
    "            if self.check_winner(i, j, marker):\n",
    "                print(f\"Player {self.player_turn} wins!\")\n",
    "                self.game_over = True\n",
    "            else:\n",
    "                self.player_turn = 1 if self.player_turn == 2 else 2\n",
    "\n",
    "    def check_winner(self, i, j, marker):\n",
    "        return (all(self.board[i][col]['text'] == marker for col in range(3)) or  # Check row\n",
    "                all(self.board[row][j]['text'] == marker for row in range(3)) or  # Check column\n",
    "                (i == j and all(self.board[index][index]['text'] == marker for index in range(3))) or  # Check main diagonal\n",
    "                (i + j == 2 and all(self.board[index][2-index]['text'] == marker for index in range(3))))  # Check other diagonal\n",
    "\n",
    "    def run(self):\n",
    "        self.root.mainloop()\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4862356d",
   "metadata": {},
   "source": [
    "# Train the model first without GUI and then use GUI during testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f7b18b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T00:01:07.200233Z",
     "start_time": "2023-05-26T23:59:11.283080Z"
    }
   },
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = [[' ' for _ in range(3)] for _ in range(3)]\n",
    "        self.player_markers = ['X', 'O']\n",
    "\n",
    "    def empty_cells(self):\n",
    "        return [(i, j) for i in range(3) for j in range(3) if self.board[i][j] == ' ']\n",
    "\n",
    "    def check_winner(self):\n",
    "        # Check rows\n",
    "        for row in self.board:\n",
    "            if len(set(row)) == 1 and row[0] != ' ':\n",
    "                return row[0]\n",
    "        # Check columns\n",
    "        for col in range(3):\n",
    "            column = [row[col] for row in self.board]\n",
    "            if len(set(column)) == 1 and column[0] != ' ':\n",
    "                return column[0]\n",
    "        # Check diagonals\n",
    "        if len(set(self.board[i][i] for i in range(3))) == 1 and self.board[0][0] != ' ':\n",
    "            return self.board[0][0]\n",
    "        if len(set(self.board[i][2-i] for i in range(3))) == 1 and self.board[0][2] != ' ':\n",
    "            return self.board[0][2]\n",
    "        # No winner\n",
    "        if ' ' in (cell for row in self.board for cell in row):  # if there's still empty cells, game continues\n",
    "            return None\n",
    "        else:\n",
    "            return 'Draw'  # If no empty cells and no winner, it's a draw\n",
    "\n",
    "\n",
    "    def random_move(self):\n",
    "        return self.empty_cells()[np.random.randint(len(self.empty_cells()))]\n",
    "\n",
    "class QLearningPlayer:\n",
    "    def __init__(self, alpha=0.5, gamma=0.9, epsilon=0.1, learning_rate=0.02, discount_factor=0.4):\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.q_table = dict()  # Q-table\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "        self.marker = 'X'  # Player's marker\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        return self.q_table.get((self.state_to_str(state), action), 0)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        empty_cells = [(i, j) for i in range(3) for j in range(3) if state[i][j] == ' ']\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Exploration: choose random action\n",
    "            action_idx = np.random.randint(len(empty_cells))\n",
    "        else:\n",
    "            # Exploitation: choose action with max Q-value\n",
    "            q_values = [self.get_q_value(state, action) for action in empty_cells]\n",
    "            action_idx = np.argmax(q_values)\n",
    "        self.state = state\n",
    "        self.action = empty_cells[action_idx]\n",
    "        return self.action\n",
    "\n",
    "\n",
    "\n",
    "    def update_q_value(self, reward, next_state):\n",
    "        current_q_value = self.get_q_value(self.state, self.action)\n",
    "        empty_cells = [(i, j) for i in range(3) for j in range(3) if next_state[i][j] == ' ']\n",
    "        if empty_cells:  # Check if there are still empty cells\n",
    "            max_next_q_value = max([self.get_q_value(next_state, action) for action in empty_cells])\n",
    "        else:\n",
    "            max_next_q_value = 0\n",
    "        new_q_value = current_q_value + self.alpha * (reward + self.gamma * max_next_q_value - current_q_value)\n",
    "        self.q_table[(self.state_to_str(self.state), self.action)] = new_q_value\n",
    "\n",
    "\n",
    "\n",
    "    def state_to_str(self, state):\n",
    "        # Change here: replace the ' ' with a neutral value like 'N'\n",
    "        return ''.join([''.join(row).replace(' ', 'N') for row in state])\n",
    "\n",
    "\n",
    "class TicTacToeQLearning(TicTacToe):\n",
    "    def __init__(self, player):\n",
    "        super().__init__()\n",
    "        self.player = player\n",
    "  \n",
    "        \n",
    "    def action_to_position(self, action):\n",
    "\t    return divmod(action, 3)\n",
    "    \n",
    "    def position_to_action(self, position):\n",
    "    \treturn position[0] * 3 + position[1]\n",
    "\n",
    "\n",
    "    def play(self):\n",
    "        while True:\n",
    "            action = self.player.choose_action(self.board)\n",
    "            i, j = action\n",
    "            self.board[i][j] = self.player.marker\n",
    "            reward = -1 if self.check_winner() == self.player_markers[1 - self.player_markers.index(self.player.marker)] else 0\n",
    "            self.player.update_q_value(reward, self.board)\n",
    "            if self.check_winner() == self.player.marker:\n",
    "                self.player.update_q_value(1, self.board)\n",
    "                break\n",
    "            elif len(self.empty_cells()) == 0:\n",
    "                break\n",
    "            else:\n",
    "                i, j = self.random_move()\n",
    "                self.board[i][j] = self.player_markers[1 - self.player_markers.index(self.player.marker)]\n",
    "                reward = 1\n",
    "\n",
    "    \n",
    "    def human_play(self):\n",
    "        while True:\n",
    "            print(self.board)\n",
    "            if self.player.marker == 'X':\n",
    "                action = int(input(\"Choose your action (0-8): \"))\n",
    "                i, j = self.action_to_position(action)\n",
    "                self.board[i][j] = self.player.marker\n",
    "                if self.check_winner() == self.player.marker:\n",
    "                    print(\"You win!\")\n",
    "                    break\n",
    "                elif len(self.empty_cells()) == 0:\n",
    "                    print(\"It's a draw!\")\n",
    "                    break\n",
    "            if self.empty_cells():  # Only choose an action if there are empty cells\n",
    "                action = self.player.choose_action(self.board)\n",
    "                if isinstance(action, tuple):  # Check if the action is a tuple before trying to convert it\n",
    "                    action = self.position_to_action(action)  # convert AI player's action choice to integer\n",
    "                i, j = self.action_to_position(action)\n",
    "                self.board[i][j] = self.player_markers[1 - self.player_markers.index(self.player.marker)]\n",
    "                if self.check_winner() == self.player_markers[1 - self.player_markers.index(self.player.marker)]:\n",
    "                    print(\"QLearningPlayer wins!\")\n",
    "                    break\n",
    "            elif len(self.empty_cells()) == 0:\n",
    "                print(\"It's a draw!\")\n",
    "                break\n",
    "\n",
    "\n",
    "class TicTacToeGUI:\n",
    "    def __init__(self, master, player):\n",
    "        self.master = master\n",
    "        self.player = player\n",
    "        self.board = TicTacToe()\n",
    "        self.buttons = [[None, None, None] for _ in range(3)]\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                self.buttons[i][j] = tk.Button(master, command=lambda row=i, col=j: self.make_move(row, col), height=3, width=6)\n",
    "                self.buttons[i][j].grid(row=i, column=j)\n",
    "\n",
    "    def make_move(self, row, col):\n",
    "        if self.board.board[row][col] == ' ':\n",
    "            self.board.board[row][col] = self.player.marker\n",
    "            self.buttons[row][col].config(text=self.player.marker)\n",
    "            winner = self.board.check_winner()\n",
    "            if winner is not None:\n",
    "                self.game_over(winner)\n",
    "            else:\n",
    "                self.ai_move()\n",
    "\n",
    "    def ai_move(self):\n",
    "        action = self.player.choose_action(self.board.board)\n",
    "        row, col = action\n",
    "        self.board.board[row][col] = self.board.player_markers[1 - self.board.player_markers.index(self.player.marker)]\n",
    "        self.buttons[row][col].config(text=self.board.player_markers[1 - self.board.player_markers.index(self.player.marker)])\n",
    "        winner = self.board.check_winner()\n",
    "        if winner is not None:\n",
    "            self.game_over(winner)\n",
    "\n",
    "    def game_over(self, winner):\n",
    "        if winner == 'Draw':\n",
    "            messagebox.showinfo(\"Game Over\", \"The game is a draw.\")\n",
    "        else:\n",
    "            messagebox.showinfo(\"Game Over\", f\"The winner is {winner}.\")\n",
    "        self.master.destroy()\n",
    "\n",
    "\n",
    "def main():\n",
    "    player = QLearningPlayer(alpha=0.5, gamma=0.6, epsilon=0.6)\n",
    "\n",
    "    # Train the QLearningPlayer\n",
    "    num_episodes = 1000000\n",
    "    for _ in range(num_episodes):\n",
    "        game = TicTacToeQLearning(player)\n",
    "        game.play()\n",
    "        player.epsilon *= 0.999\n",
    "\n",
    "    # Save the trained player to a file\n",
    "    with open('trained_player.pkl', 'wb') as f:\n",
    "        pickle.dump(player, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82fb3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe6f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing is below... Above code is for training which worked pretty well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4843337",
   "metadata": {},
   "source": [
    "### Now I will load the pickle file and test the model in GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85243d06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T00:44:24.971316Z",
     "start_time": "2023-05-27T00:44:15.062325Z"
    }
   },
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = [[' ' for _ in range(3)] for _ in range(3)]\n",
    "        self.player_markers = ['X', 'O']\n",
    "\n",
    "    def empty_cells(self):\n",
    "        return [(i, j) for i in range(3) for j in range(3) if self.board[i][j] == ' ']\n",
    "\n",
    "    def check_winner(self):\n",
    "        # Check rows\n",
    "        for row in self.board:\n",
    "            if len(set(row)) == 1 and row[0] != ' ':\n",
    "                return row[0]\n",
    "        # Check columns\n",
    "        for col in range(3):\n",
    "            column = [row[col] for row in self.board]\n",
    "            if len(set(column)) == 1 and column[0] != ' ':\n",
    "                return column[0]\n",
    "        # Check diagonals\n",
    "        if len(set(self.board[i][i] for i in range(3))) == 1 and self.board[0][0] != ' ':\n",
    "            return self.board[0][0]\n",
    "        if len(set(self.board[i][2-i] for i in range(3))) == 1 and self.board[0][2] != ' ':\n",
    "            return self.board[0][2]\n",
    "        # No winner\n",
    "        if ' ' in (cell for row in self.board for cell in row):  # if there's still empty cells, game continues\n",
    "            return None\n",
    "        else:\n",
    "            return 'Draw'  # If no empty cells and no winner, it's a draw\n",
    "\n",
    "\n",
    "    def random_move(self):\n",
    "        return self.empty_cells()[np.random.randint(len(self.empty_cells()))]\n",
    "\n",
    "class QLearningPlayer:\n",
    "    def __init__(self, alpha=0.5, gamma=0.9, epsilon=0.1, learning_rate=0.02, discount_factor=0.4):\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.q_table = dict()  # Q-table\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "        self.marker = 'X'  # Player's marker\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        return self.q_table.get((self.state_to_str(state), action), 0)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        empty_cells = [(i, j) for i in range(3) for j in range(3) if state[i][j] == ' ']\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Exploration: choose random action\n",
    "            action_idx = np.random.randint(len(empty_cells))\n",
    "        else:\n",
    "            # Exploitation: choose action with max Q-value\n",
    "            q_values = [self.get_q_value(state, action) for action in empty_cells]\n",
    "            action_idx = np.argmax(q_values)\n",
    "        self.state = state\n",
    "        self.action = empty_cells[action_idx]\n",
    "        return self.action\n",
    "\n",
    "\n",
    "\n",
    "    def update_q_value(self, reward, next_state):\n",
    "        current_q_value = self.get_q_value(self.state, self.action)\n",
    "        empty_cells = [(i, j) for i in range(3) for j in range(3) if next_state[i][j] == ' ']\n",
    "        if empty_cells:  # Check if there are still empty cells\n",
    "            max_next_q_value = max([self.get_q_value(next_state, action) for action in empty_cells])\n",
    "        else:\n",
    "            max_next_q_value = 0\n",
    "        new_q_value = current_q_value + self.alpha * (reward + self.gamma * max_next_q_value - current_q_value)\n",
    "        self.q_table[(self.state_to_str(self.state), self.action)] = new_q_value\n",
    "\n",
    "\n",
    "\n",
    "    def state_to_str(self, state):\n",
    "        # Change here: replace the ' ' with a neutral value like 'N'\n",
    "        return ''.join([''.join(row).replace(' ', 'N') for row in state])\n",
    "\n",
    "\n",
    "class TicTacToeQLearning(TicTacToe):\n",
    "    def __init__(self, player):\n",
    "        super().__init__()\n",
    "        self.player = player\n",
    "  \n",
    "        \n",
    "    def action_to_position(self, action):\n",
    "\t    return divmod(action, 3)\n",
    "    \n",
    "    def position_to_action(self, position):\n",
    "    \treturn position[0] * 3 + position[1]\n",
    "\n",
    "\n",
    "    def play(self):\n",
    "        while True:\n",
    "            action = self.player.choose_action(self.board)\n",
    "            i, j = action\n",
    "            self.board[i][j] = self.player.marker\n",
    "            reward = -1 if self.check_winner() == self.player_markers[1 - self.player_markers.index(self.player.marker)] else 0\n",
    "            self.player.update_q_value(reward, self.board)\n",
    "            if self.check_winner() == self.player.marker:\n",
    "                self.player.update_q_value(1, self.board)\n",
    "                break\n",
    "            elif len(self.empty_cells()) == 0:\n",
    "                break\n",
    "            else:\n",
    "                i, j = self.random_move()\n",
    "                self.board[i][j] = self.player_markers[1 - self.player_markers.index(self.player.marker)]\n",
    "                reward = 1\n",
    "\n",
    "    \n",
    "    def human_play(self):\n",
    "        while True:\n",
    "            print(self.board)\n",
    "            if self.player.marker == 'X':\n",
    "                action = int(input(\"Choose your action (0-8): \"))\n",
    "                i, j = self.action_to_position(action)\n",
    "                self.board[i][j] = self.player.marker\n",
    "                if self.check_winner() == self.player.marker:\n",
    "                    print(\"You win!\")\n",
    "                    break\n",
    "                elif len(self.empty_cells()) == 0:\n",
    "                    print(\"It's a draw!\")\n",
    "                    break\n",
    "            if self.empty_cells():  # Only choose an action if there are empty cells\n",
    "                action = self.player.choose_action(self.board)\n",
    "                if isinstance(action, tuple):  # Check if the action is a tuple before trying to convert it\n",
    "                    action = self.position_to_action(action)  # convert AI player's action choice to integer\n",
    "                i, j = self.action_to_position(action)\n",
    "                self.board[i][j] = self.player_markers[1 - self.player_markers.index(self.player.marker)]\n",
    "                if self.check_winner() == self.player_markers[1 - self.player_markers.index(self.player.marker)]:\n",
    "                    print(\"QLearningPlayer wins!\")\n",
    "                    break\n",
    "            elif len(self.empty_cells()) == 0:\n",
    "                print(\"It's a draw!\")\n",
    "                break\n",
    "\n",
    "\n",
    "class TicTacToeGUI:\n",
    "    def __init__(self, master, player):\n",
    "        self.master = master\n",
    "        self.player = player\n",
    "        self.board = TicTacToe()\n",
    "        self.buttons = [[None, None, None] for _ in range(3)]\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                self.buttons[i][j] = tk.Button(master, command=lambda row=i, col=j: self.make_move(row, col), height=3, width=6)\n",
    "                self.buttons[i][j].grid(row=i, column=j)\n",
    "\n",
    "    def make_move(self, row, col):\n",
    "        if self.board.board[row][col] == ' ':\n",
    "            self.board.board[row][col] = self.player.marker\n",
    "            self.buttons[row][col].config(text=self.player.marker)\n",
    "            winner = self.board.check_winner()\n",
    "            if winner is not None:\n",
    "                self.game_over(winner)\n",
    "            else:\n",
    "                self.ai_move()\n",
    "\n",
    "    def ai_move(self):\n",
    "        action = self.player.choose_action(self.board.board)\n",
    "        row, col = action\n",
    "        self.board.board[row][col] = self.board.player_markers[1 - self.board.player_markers.index(self.player.marker)]\n",
    "        self.buttons[row][col].config(text=self.board.player_markers[1 - self.board.player_markers.index(self.player.marker)])\n",
    "        winner = self.board.check_winner()\n",
    "        if winner is not None:\n",
    "            self.game_over(winner)\n",
    "\n",
    "    def game_over(self, winner):\n",
    "        if winner == 'Draw':\n",
    "            messagebox.showinfo(\"Game Over\", \"The game is a draw.\")\n",
    "        else:\n",
    "            messagebox.showinfo(\"Game Over\", f\"The winner is {winner}.\")\n",
    "        self.master.destroy()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # player = QLearningPlayer(alpha=0.5, gamma=0.6, epsilon=0.6)\n",
    "\t\n",
    "    # Load the trained player from file. \n",
    "    with open('./trained_player.pkl', 'rb') as f:\n",
    "        player = pickle.load(f)\n",
    "\n",
    "    # Switch to exploitation mode\n",
    "    player.epsilon = 0  \n",
    "\n",
    "    # Play against human\n",
    "    root = tk.Tk()\n",
    "    gui = TicTacToeGUI(root, player)\n",
    "    root.mainloop()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ed21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The above code works well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5ee39",
   "metadata": {},
   "source": [
    "# Ok now I am gonna change the reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a0c7f55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T00:34:47.316489Z",
     "start_time": "2023-05-27T00:32:44.496211Z"
    }
   },
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = [[' ' for _ in range(3)] for _ in range(3)]\n",
    "        self.player_markers = ['X', 'O']\n",
    "\n",
    "    def empty_cells(self):\n",
    "        return [(i, j) for i in range(3) for j in range(3) if self.board[i][j] == ' ']\n",
    "\n",
    "    def check_winner(self):\n",
    "        # Check rows\n",
    "        for row in self.board:\n",
    "            if len(set(row)) == 1 and row[0] != ' ':\n",
    "                return row[0]\n",
    "        # Check columns\n",
    "        for col in range(3):\n",
    "            column = [row[col] for row in self.board]\n",
    "            if len(set(column)) == 1 and column[0] != ' ':\n",
    "                return column[0]\n",
    "        # Check diagonals\n",
    "        if len(set(self.board[i][i] for i in range(3))) == 1 and self.board[0][0] != ' ':\n",
    "            return self.board[0][0]\n",
    "        if len(set(self.board[i][2-i] for i in range(3))) == 1 and self.board[0][2] != ' ':\n",
    "            return self.board[0][2]\n",
    "        # No winner\n",
    "        if ' ' in (cell for row in self.board for cell in row):  # if there's still empty cells, game continues\n",
    "            return None\n",
    "        else:\n",
    "            return 'Draw'  # If no empty cells and no winner, it's a draw\n",
    "\n",
    "\n",
    "    def random_move(self):\n",
    "        return self.empty_cells()[np.random.randint(len(self.empty_cells()))]\n",
    "\n",
    "class QLearningPlayer:\n",
    "    def __init__(self, alpha=0.5, gamma=0.9, epsilon=0.1, learning_rate=0.02, discount_factor=0.4):\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.q_table = dict()  # Q-table\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "        self.marker = 'X'  # Player's marker\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        return self.q_table.get((self.state_to_str(state), action), 0)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        empty_cells = [(i, j) for i in range(3) for j in range(3) if state[i][j] == ' ']\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Exploration: choose random action\n",
    "            action_idx = np.random.randint(len(empty_cells))\n",
    "        else:\n",
    "            # Exploitation: choose action with max Q-value\n",
    "            q_values = [self.get_q_value(state, action) for action in empty_cells]\n",
    "            action_idx = np.argmax(q_values)\n",
    "        self.state = state\n",
    "        self.action = empty_cells[action_idx]\n",
    "        return self.action\n",
    "\n",
    "    def update_q_value(self, reward, next_state):\n",
    "        current_q_value = self.get_q_value(self.state, self.action)\n",
    "        empty_cells = [(i, j) for i in range(3) for j in range(3) if next_state[i][j] == ' ']\n",
    "        if empty_cells:  \n",
    "            max_next_q_value = max([self.get_q_value(next_state, action) for action in empty_cells])\n",
    "        else:\n",
    "            max_next_q_value = 0\n",
    "        new_q_value = current_q_value + self.alpha * (reward + self.gamma * max_next_q_value - current_q_value)\n",
    "        self.q_table[(self.state_to_str(self.state), self.action)] = new_q_value\n",
    "\n",
    "    def state_to_str(self, state):\n",
    "        # Change here: replace the ' ' with a neutral value like 'N'\n",
    "        return ''.join([''.join(row).replace(' ', 'N') for row in state])\n",
    "\n",
    "\n",
    "class TicTacToeQLearning(TicTacToe):\n",
    "    def __init__(self, player):\n",
    "        super().__init__()\n",
    "        self.player = player\n",
    "  \n",
    "        \n",
    "    def action_to_position(self, action):\n",
    "\t    return divmod(action, 3)\n",
    "    \n",
    "    def position_to_action(self, position):\n",
    "    \treturn position[0] * 3 + position[1]\n",
    "\n",
    "    def play(self):\n",
    "        while True:\n",
    "            action = self.player.choose_action(self.board)\n",
    "            i, j = action\n",
    "            self.board[i][j] = self.player.marker\n",
    "            if self.check_winner() == self.player_markers[1 - self.player_markers.index(self.player.marker)]:\n",
    "                reward = -100\n",
    "            elif self.check_winner() == self.player.marker:\n",
    "                reward = 100\n",
    "            elif len(self.empty_cells()) == 0:\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = 10\n",
    "            self.player.update_q_value(reward, self.board)\n",
    "            if self.check_winner() == self.player.marker:\n",
    "                reward = 100  # give the player a reward for winning\n",
    "                self.player.update_q_value(reward, self.board)\n",
    "                break\n",
    "            elif len(self.empty_cells()) == 0:\n",
    "                break\n",
    "            else:\n",
    "                i, j = self.random_move()\n",
    "                self.board[i][j] = self.player_markers[1 - self.player_markers.index(self.player.marker)]\n",
    "    \n",
    "    def human_play(self):\n",
    "        while True:\n",
    "            print(self.board)\n",
    "            if self.player.marker == 'X':\n",
    "                action = int(input(\"Choose your action (0-8): \"))\n",
    "                i, j = self.action_to_position(action)\n",
    "                self.board[i][j] = self.player.marker\n",
    "                if self.check_winner() == self.player.marker:\n",
    "                    print(\"You win!\")\n",
    "                    break\n",
    "                elif len(self.empty_cells()) == 0:\n",
    "                    print(\"It's a draw!\")\n",
    "                    break\n",
    "            if self.empty_cells():  # Only choose an action if there are empty cells\n",
    "                action = self.player.choose_action(self.board)\n",
    "                if isinstance(action, tuple):  # Check if the action is a tuple before trying to convert it\n",
    "                    action = self.position_to_action(action)  # convert AI player's action choice to integer\n",
    "                i, j = self.action_to_position(action)\n",
    "                self.board[i][j] = self.player_markers[1 - self.player_markers.index(self.player.marker)]\n",
    "                if self.check_winner() == self.player_markers[1 - self.player_markers.index(self.player.marker)]:\n",
    "                    print(\"QLearningPlayer wins!\")\n",
    "                    break\n",
    "            elif len(self.empty_cells()) == 0:\n",
    "                print(\"It's a draw!\")\n",
    "                break\n",
    "\n",
    "class TicTacToeGUI:\n",
    "    def __init__(self, master, player):\n",
    "        self.master = master\n",
    "        self.player = player\n",
    "        self.board = TicTacToe()\n",
    "        self.buttons = [[None, None, None] for _ in range(3)]\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                self.buttons[i][j] = tk.Button(master, command=lambda row=i, col=j: self.make_move(row, col), height=3, width=6)\n",
    "                self.buttons[i][j].grid(row=i, column=j)\n",
    "\n",
    "    def make_move(self, row, col):\n",
    "        if self.board.board[row][col] == ' ':\n",
    "            self.board.board[row][col] = self.player.marker\n",
    "            self.buttons[row][col].config(text=self.player.marker)\n",
    "            winner = self.board.check_winner()\n",
    "            if winner is not None:\n",
    "                self.game_over(winner)\n",
    "            else:\n",
    "                self.ai_move()\n",
    "\n",
    "    def ai_move(self):\n",
    "        action = self.player.choose_action(self.board.board)\n",
    "        row, col = action\n",
    "        self.board.board[row][col] = self.board.player_markers[1 - self.board.player_markers.index(self.player.marker)]\n",
    "        self.buttons[row][col].config(text=self.board.player_markers[1 - self.board.player_markers.index(self.player.marker)])\n",
    "        winner = self.board.check_winner()\n",
    "        if winner is not None:\n",
    "            self.game_over(winner)\n",
    "\n",
    "    def game_over(self, winner):\n",
    "        if winner == 'Draw':\n",
    "            messagebox.showinfo(\"Game Over\", \"The game is a draw.\")\n",
    "        else:\n",
    "            messagebox.showinfo(\"Game Over\", f\"The winner is {winner}.\")\n",
    "        self.master.destroy()\n",
    "\n",
    "\n",
    "def main():\n",
    "    player = QLearningPlayer(alpha=0.5, gamma=0.6, epsilon=0.8)\n",
    "\n",
    "    # Train the QLearningPlayer\n",
    "    num_episodes = 1000000\n",
    "    decay_rate = 0.9999  # Make the decay slower\n",
    "    for _ in range(num_episodes):\n",
    "        game = TicTacToeQLearning(player)\n",
    "        game.play()\n",
    "        player.epsilon *= decay_rate\n",
    "\n",
    "    # Save the trained player to a file\n",
    "    with open('trained_player.pkl', 'wb') as f:\n",
    "        pickle.dump(player, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9e4b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c070b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d357915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2460e6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822bec79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bde61f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d7a257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedb0f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593726ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1b3cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
